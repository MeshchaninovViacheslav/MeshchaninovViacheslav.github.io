<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Continuous Diffusion Model for Language Modeling | Viacheslav Meshchaninov </title> <meta name="author" content="Viacheslav Meshchaninov"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://meshchaninovviacheslav.github.io/notes/natural%20language%20processing/text%20diffusion%20generation/jo2025continuous/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap" rel="stylesheet"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top py-3 py-md-4" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-bold" href="/" style="font-size: 1.125rem; letter-spacing: -0.025em;"> <span class="font-weight-bold">Viacheslav</span> Meshchaninov </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link text-sm font-weight-medium ml-md-4 " href="/">about </a> </li> <li class="nav-item "> <a class="nav-link text-sm font-weight-medium ml-md-4 " href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link text-sm font-weight-medium ml-md-4 text-primary" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link text-sm font-weight-medium ml-md-4 " href="/cv/">cv </a> </li> <li class="nav-item ml-md-4"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container ml-md-4"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <article class="post"> <header class="post-header"> <h1 class="post-title">Continuous Diffusion Model for Language Modeling</h1> <div class="post-authors text-muted"></div> <div class="post-venue text-muted font-italic mb-3"> </div> <p class="post-meta">February 17, 2025</p> <div class="buttons mt-3 mb-4"> <a class="btn btn-primary" href="https://arxiv.org/pdf/2502.11564" role="button" target="_blank" rel="noopener noreferrer"> <i class="fa-solid fa-file-pdf"></i> PDF </a> <a class="btn btn-primary" href="https://github.com/harryjo97/RDLM" role="button" target="_blank" rel="noopener noreferrer"> <i class="fa-brands fa-github"></i> Code </a> </div> <div class="post-tags"> <span class="badge badge-secondary">text-diffusion</span> <span class="badge badge-secondary">rdlm</span> </div> </header> <div class="post-content mt-4"> <h2 id="methodology">Methodology</h2> <hr> <h3 id="preliminaries">Preliminaries</h3> <h4 id="statistical-manifold-of-categorical-distribution">Statistical manifold of categorical distribution</h4> <ol> <li>Let \(\mathcal{X} = \{1, \cdots, d\}\) denote the discrete data space</li> <li>\(\Delta^{d-1} = \{(p_1, \cdots, p_d) \in \mathbb{R}^d \mid \sum_i p_i = 1, p_i \geq 0\}\) denote the $(d - 1)$-dimensional probability simplex.</li> </ol> <p>A natural choice of a Riemannian metric on the simplex is the Fisher–Rao metric. For an interior point $p \in \Delta^{d-1}$, the Fisher–Rao metric is defined as follows:</p> \[\begin{equation} g_{FR}(p)[x, y] := \langle x, y \rangle_p := \left\langle \frac{x}{\sqrt{p}}, \frac{y}{\sqrt{p}} \right\rangle = \sum_{i=1}^d \frac{x_i y_i}{p_i}, \quad x, y \in \mathcal{T}_p \Delta^{d-1}, \end{equation}\] <p>This induces a geodesic distance on the simplex defined as follows:</p> \[\begin{equation} d(p, q) = 2 \cos^{-1} \left( \sum_{i=1}^d \sqrt{p_i q_i} \right), \quad p, q \in \Delta^{d-1}, \end{equation}\] <p>where $p$ and $q$ correspond to the parameters of categorical distributions.</p> <p><br></p> <blockquote> <p>The probability simplex $\Delta^{d-1}$ equipped with the Fisher–Rao metric is a Riemannian manifold called the statistical manifold of categorical distribution, denoted as $\mathcal{P}(\mathcal{X})$ throughout the paper.</p> </blockquote> <h4 id="hypersphere">Hypersphere</h4> <ul> <li>\(\mathbb{S}^{d-1}\) denotes the $(d-1)$-dimensional sphere \(\bigg\{ \mathbf{u} = (u_1, \cdots, u_d) : \sum_i u_i^2 = 1 \bigg\}\)</li> <li>\(\mathbb{S}^{d-1}_+ = \bigg\{ \mathbf{u} = (u_1, \cdots, u_d) : \sum_i u_i^2 = 1, u_i \geq 0 \bigg\}\) denotes a positive orthant of \(\mathbb{S}^{d-1}\).</li> </ul> <p>For a discrete sample space \(\mathcal{X} = \{1, 2, \cdots, d\}\), there exists a diffeomorphism (isomorphism of differentiable manifolds) from $\mathcal{P}(\mathcal{X})$ to $\mathbb{S}^{d-1}_+$ defined as follows:</p> \[\begin{align*} \pi : \mathcal{P}(\mathcal{X}) \rightarrow \mathbb{S}^{d-1}_+ ; \quad p_i \mapsto u_i = \sqrt{p_i}, \newline \pi^{-1} : \mathbb{S}^{d-1}_+ \rightarrow \mathcal{P}(\mathcal{X}) ; \quad u_i \mapsto p_i = u_i^2. \end{align*}\] <p>The diffeomorphism induces the geodesic distance (minimal distance between two points on hypersphere) on $\mathbb{S}^{d-1}_+$:</p> \[\begin{equation} d_g(\mathbf{u}, \mathbf{v}) = \cos^{-1} \langle \mathbf{u}, \mathbf{v} \rangle, \quad \mathbf{u}, \mathbf{v} \in \mathbb{S}^{d-1}_+, \end{equation}\] <p>The corresponding exponential and logarithm maps on $\mathbb{S}^{d-1}$ can be computed as follows:</p> <p>\begin{equation} \exp<em>{\mathbf{u}} \mathbf{x} = \cos(|\mathbf{x}|) \mathbf{u} + \sin(|\mathbf{x}|) \frac{\mathbf{x}}{|\mathbf{x}|}, \quad \mathbf{u} \in \mathbb{S}^{d-1}, \ \mathbf{x} \in T</em>{\mathbf{u}}(\mathbb{S}^{d-1}), \end{equation}</p> <p>\begin{equation} \exp_{\mathbf{u}}^{-1}(\mathbf{v}) = \frac{\cos^{-1} \langle \mathbf{u}, \mathbf{v} \rangle} {\sqrt{1 - \langle \mathbf{u}, \mathbf{v} \rangle^2}} \left( \mathbf{v} - \langle \mathbf{u}, \mathbf{v} \rangle \mathbf{u} \right), \quad \mathbf{u}, \mathbf{v} \in \mathbb{S}^{d-1}. \end{equation}</p> <p>Basically this formulas define the projection rule between initial space $\mathbb{S}^{d-1}$ and its tangent space \(T_{\mathbf{u}}(\mathbb{S}^{d-1})\).</p> <blockquote> <p>The motivation of mapping \(\mathcal{P}(\mathcal{X})\) to the positive orthant of a hypersphere \(\mathbb{S}_+^{d-1}\) is that the Fisher–Rao metric is ill-defined on the boundary of the manifold where the initial distribution of the parameterized data lies. So the task of modeling the distribution of discrete data can be reformulated as modeling a distribution $p_{\text{data}}$ on the hypersphere.</p> </blockquote> <h3 id="generative-process-on-hypersphere">Generative Process on Hypersphere</h3> <p>On a general manifold $\mathcal{M}$ (complete, orientable, connected, and boundaryless) the logarithm bridge process from $x_0 \in \mathcal{M}$ to $x_1 \in \mathcal{M}$ is defined as follows:</p> \[\begin{equation} \mathrm{d}\bar{X}_t = \gamma_t \exp^{-1}_{\bar{X}_t}(x_1)\mathrm{d}t + \sigma_t \mathrm{d}B_t^{\mathcal{M}}, \quad \bar{X}_0 = x_0, \quad \gamma_t := \frac{\sigma_t^2}{\int_t^T \sigma_s^2 \mathrm{d}s} \end{equation}\] <p>where $\exp^{-1}_x(\cdot)$ denotes the logarithm map on $\mathcal{M}$ at point $x$, and $B_t^{\mathcal{M}}$ is the Brownian motion defined on $\mathcal{M}$.</p> <details class="details-frame"> <summary>Interpretation of the Equation </summary> <p>Even though \(\bar{X}_t \in \mathcal{M}\), the SDE is written <strong>in the tangent space</strong> \(T_{\bar{X}_t} \mathcal{M}\).</p> <p>In differential geometry, this is a standard trick: <strong>define dynamics in the tangent space</strong>, then use <strong>the exponential map</strong> (or a retraction) to interpret these dynamics as evolving on the manifold.</p> <p>This is especially necessary in <strong>manifold-valued stochastic processes</strong>, where there’s no global coordinate system. So what actually happens here is:</p> <blockquote> <p>The SDE is computed locally in $T_{\bar{X}_t} \mathcal{M}$, and $\bar{X}_t$ evolves by lifting the update back to the manifold.</p> </blockquote> <p>Let’s rewrite the core:</p> \[\mathrm{d}\bar{X}_t = \gamma_t \exp^{-1}_{\bar{X}_t}(x_1)\, \mathrm{d}t + \sigma_t\, \mathrm{d}B_t^{\mathcal{M}}\] <p>This defines a <strong>logarithmic bridge</strong>: a stochastic process starting at $x_0 \in \mathcal{M}$, gently “pulled” toward $x_1 \in \mathcal{M}$ using the <strong>logarithmic map</strong> as a direction.</p> <ul> <li>$\exp^{-1}_{\bar{X}_t}(x_1)$ gives the <strong>direction and magnitude</strong> in which to pull toward the target.</li> <li>$\gamma_t$ adjusts the strength of this drift over time.</li> <li>$\sigma_t\, \mathrm{d}B_t^{\mathcal{M}}$ is Brownian motion noise <strong>on the manifold</strong>, typically modeled via parallel transport or using local charts.</li> </ul> </details> <p>In the case of $\mathcal{M} = \mathbb{S}^{d-1}$, we can derive the logarithm bridge process from $x_0$ to $e_k$ using formula (5): \begin{equation} \mathrm{d}\bar{X}_t = \gamma_t \frac{\cos^{-1} \langle \bar{X}_t, e_k \rangle (e_k - \langle \bar{X}_t, e_k \rangle \bar{X}_t)} {\sqrt{1 - \langle \bar{X}_t, e_k \rangle^2}} \mathrm{d}t + \sigma_t \mathrm{d}B_t^d, \quad \bar{X}_0 = x_0 \end{equation}</p> <h3 id="diffusion-mixture-representation">Diffusion Mixture Representation</h3> <blockquote> <p>From the logarithm bridge processes (Eq.(7)), the authors construct a generative process \(\{X_t\}_{t=0}^T\) on \(\mathbb{S}^{d-1}\) using the diffusion mixture representation.</p> </blockquote> <p>Let $p_{\text{data}}(x) = \sum_{k=1}^d p_k \delta(x - e_k)$ be a data distribution on $\mathbb{S}^{d-1}$. Then the following SDE defines a diffusion process that transports the initial point $x_0 \in \mathbb{S}^{d-1}$ to the distribution $p_{\text{data}}$:</p> \[\begin{equation} \mathrm{d}X_t = \left[ \sum_{k=1}^d p_{T|t}(e_k \mid X_t) \, \eta^k(X_t, t) \right] \mathrm{d}t + \sigma_t \mathrm{d}B_t^d, \quad X_0 = x_0, \end{equation}\] \[\begin{equation} \eta^k(z, t) := \gamma_t \frac{ \cos^{-1} \langle z, e_k \rangle (e_k - \langle z, e_k \rangle z) } { \sqrt{1 - \langle z, e_k \rangle^2} }, \end{equation}\] <p>where $p_{T|t}(e_k \mid X_t)$ represents the conditional probability that the process will reach the endpoint $e_k$ at time $T$, given the current state $X_t$ at time $t$.</p> <p><br> The authors derive a new family of generative processes by constructing a mixture over the time marginals of generative processes \(\{ \mathbb{Q}_t^i : 1 \leq i \leq n \}\):</p> \[\begin{equation} \mathbb{Q}_t^{\text{mix}} := \sum_{i=1}^n \lambda_t^i \mathbb{Q}_t^i, \quad \sum_{i=1}^n \lambda_t^i = 1, \quad 0 \leq \lambda_t^i \leq 1, \end{equation}\] <p>where $\lambda_t^i$ is the time-dependent mixing schedule assigned to the $i$-th generative path. This construction allows the resulting process to transition between different generative behaviors over time.</p> <p>In particular, the authors propose a simple mixture path built from mixing the time marginals of the masked diffusion and uniform diffusion, for a time-dependent schedule $\lambda_t$ as follows:</p> <p>\begin{equation} \lambda_t \mathbb{Q}_t^{\text{mask}} + (1 - \lambda_t) \mathbb{Q}_t^{\text{unif}}, \end{equation}</p> <p>with initial distribution $\lambda_0 \delta(e_m) + (1 - \lambda_0) \delta\left( \sum_{i=1}^d e_i / \sqrt{d} \right)$.</p> <h3 id="training">Training</h3> <h4 id="model-parameterization">Model parameterization</h4> <p>Instead of approximating the drift function directly, the authors model the probability \(p_{T \mid t}(X_T \mid X_t)\) with a neural network $s_\theta$ as follows:</p> \[\begin{equation} p_\theta(X_t, t) := \texttt{softmax}(s_\theta(X_t, t)) = \left[ p_{T|t}(e_1 \mid X_t), \cdots, p_{T|t}(e_d \mid X_t) \right]^\top, \end{equation}\] <p>In the case of masked diffusion, we set the probability $p_{T|t}(e_m \mid X_t)$ to be zero for all $t$, indicating that the final state cannot be a mask token. From Eq.(12), the drift of the mixture process in Eq.(8) is parameterized as follows:</p> <p>\begin{equation} \eta<em>\theta(X_t, t) = \sum</em>{k=1}^d \left\langle p_\theta(X_t, t), e_k \right\rangle \eta^k(X_t, t). \end{equation}</p> <h4 id="likelihood-bound">Likelihood bound</h4> <p><strong>Variational upper bound:</strong></p> <p>Let $\mathbb{Q}^k$ be a bridge process with starting point $x_0$ and endpoint $e_k$. From the KL divergence between $\mathbb{Q}^\theta$ and $\mathbb{Q}^k$, a point-wise upper bound on the negative log-likelihood could be derived:</p> \[\begin{equation} - \log \hat{p}_\theta(e_k) = D_{\text{KL}}(\mathbb{Q}_{T}^k \| \mathbb{Q}^\theta_T) \leq \mathbb{E}_{X \sim \mathbb{Q}^k} \left[ \frac{1}{2} \int_0^T \left\| \sigma_t^{-1} \left( \eta_\theta(X_t, t) - \eta^k(X_t, t) \right) \right\|_2^2 \mathrm{d}t \right] \end{equation}\] <p><strong>Objective:</strong></p> <p>Using Eq.(13) the following objective could be derived:</p> \[\begin{equation} \mathcal{L}(\theta) = \mathbb{E}_{e_k \sim p_{\text{data}} \atop \mathbf{X} \sim \mathbb{Q}^k} \left[ \frac{1}{2} \int_0^T \sigma_t^{-2} \left\| \sum_{l=1}^d \left\langle p_\theta(\mathbf{X}_t, t), \mathbf{e}_l \right\rangle \eta^l(\mathbf{X}_t, t) - \eta^k(\mathbf{X}_t, t) \right\|_2^2 \, \mathrm{d}t \right]. \end{equation}\] <p>This formula could be further simplified:</p> \[\begin{equation} \mathcal{L}(\theta) = \mathbb{E}_{e_k \sim p_{\text{data}} \atop \mathbf{X} \sim \mathbb{Q}^k} \left[ \frac{1}{2} \int_0^T \sigma_t^{-2} \left\| \sum_{l=1}^d \left\langle p_\theta(\mathbf{X}_t, t) - \mathbf{e}_k, \mathbf{e}_l \right\rangle \eta^l(\mathbf{X}_t, t) \right\|_2^2 \, \mathrm{d}t \right]. \end{equation}\] <p>Futhermore, in the Appendix the authors show that this objective could be upperbounded by cross-entropy objective:</p> \[\begin{equation} \mathcal{L}^{CE}(\theta) = \mathbb{E}_{e_k \sim p_{\text{data}} \atop \mathbf{X} \sim \mathbb{Q}^k} \left[ \int_0^T -\log \left\langle p_\theta(\mathbf{X}_t, t), \mathbf{e}_k \right\rangle \mathrm{d}t \right] \end{equation}\] <blockquote> <p>The authors experimentally find that the cross-entropy loss yields faster convergence in training and leads to better performance than the mean squared error loss.</p> </blockquote> <h4 id="approximation-of-transition-distribution">Approximation of transition distribution</h4> <p>Training objective involves sampling $x_t$ at each training iteration. This introduces a significant bottleneck during training, as it requires simulating the process.</p> <p>The authors propose to approximate the manifold distribution $p(x_t \mid x_0, x_T)$ as the image of a Gaussian distribution on the tangent space via the exponential map.</p> <h3 id="modeling-sequence-of-tokens">Modeling sequence of tokens</h3> <p>Basically in the same manner as token process the authors model sequence process.</p> <h4 id="dimension-splitting">Dimension splitting</h4> <p>Additionally, the authors say that modeling texts with large vocabulary is challenging for neural network. That’s why they use dimmension splitting. Basically, they represent the whole probability vector of size $d$ as $m$ probablity vectors of size $b$, $b = \log_{m} d$. As I understood they just use $m$ softmax heads after neural network. This makes neural networks training significantly easier.</p> <div class="bibtex-toggle"> <a href="#" class="bib-toggle-link">view bibtex</a> </div> <div class="bibtex-content" style="display: none"> <pre><code>@article{jo2025continuous,
  title={Continuous Diffusion Model for Language Modeling},
  author={Jo, Jaehyeong and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2502.11564},
  year={2025}
}</code></pre> </div> </div> </article> <script>
  document.addEventListener("DOMContentLoaded", function () {
    const toggleLink = document.querySelector(".bib-toggle-link");
    if (toggleLink) {
      toggleLink.addEventListener("click", function (e) {
        e.preventDefault();
        const bibContent = document.querySelector(".bibtex-content");
        if (bibContent.style.display === "none") {
          bibContent.style.display = "block";
          toggleLink.textContent = "hide bibtex";
        } else {
          bibContent.style.display = "none";
          toggleLink.textContent = "view bibtex";
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Viacheslav Meshchaninov. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: July 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>